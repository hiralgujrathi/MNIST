{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MNIST.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMqLndiqwTLSsv0GhpWvePD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0932752fce5a49319a4fac877a04e037":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a2d63bff96f8469598c6ac0da79a0439","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b66569ab0ba04aa899272dbc9b3130a5","IPY_MODEL_7c737b8e7a7945deab0ecd5fbb0bda72","IPY_MODEL_9cdd318cece241968acfd7f078f1941f"]}},"a2d63bff96f8469598c6ac0da79a0439":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b66569ab0ba04aa899272dbc9b3130a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fd8e71368ce74000a7ae3da016e07cbb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Dl Completed...: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3c1c849ea964488bb93624bf46b95463"}},"7c737b8e7a7945deab0ecd5fbb0bda72":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6d6d4bbff3be4e639aad58bcaef2f07a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":4,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":4,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_24fd86990ac242959fbed31c6962ace7"}},"9cdd318cece241968acfd7f078f1941f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fca7ba5851014c27973fe79f6de52fc2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4/4 [00:00&lt;00:00,  7.39 file/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_25757877d7dc4941bb6cc0ad6aa06ebb"}},"fd8e71368ce74000a7ae3da016e07cbb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3c1c849ea964488bb93624bf46b95463":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6d6d4bbff3be4e639aad58bcaef2f07a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"24fd86990ac242959fbed31c6962ace7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fca7ba5851014c27973fe79f6de52fc2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"25757877d7dc4941bb6cc0ad6aa06ebb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"80OLsyIlPFVq"},"outputs":[],"source":[""]},{"cell_type":"code","source":[""],"metadata":{"id":"aVEqO0FMPR64"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"0trJmd6DjqBZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645509464342,"user_tz":-330,"elapsed":2079,"user":{"displayName":"Hiral Gujrathi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04049263581643961458"}},"outputId":"712b39a6-bc8c-4a44-d09d-e0753ca6ade2"},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 2.8.0\n"]}],"source":["import tensorflow as tf\n","print(\"TensorFlow version:\", tf.__version__)"]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","#tensorflow data provider as mnist thats why we import tensorflow datasets basically to load the mnist dataset\n","import tensorflow_datasets as tfds\n","#tensorflow datasets have a large number of datasets ready for modelling\n"],"metadata":{"id":"3xBPYgVlStM-","executionInfo":{"status":"ok","timestamp":1645509468239,"user_tz":-330,"elapsed":962,"user":{"displayName":"Hiral Gujrathi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04049263581643961458"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["mnist_dataset,mnist_info=tfds.load(name='mnist',with_info=True,as_supervised=True)\n","#as_supervisedt his loads the dataset in the form of inputs and targets,#with_info gives a tuple containing version and features of dataset\n","# as_supervised=True will load the dataset in a 2-tuple structure (input, target) \n","# alternatively, as_supervised=False, would return a dictionary\n","#mnist dataset in tensorflow has alreday train and test dataset so some amount or some percent of train dataset is taken for validation by us like 10%\n","mnist_train,mnist_test=mnist_dataset['train'],mnist_dataset['test']\n","#now we need to extract 10% of training data  that happens with mnist_info.splits\n","num_validation_samples=0.1*mnist_info.splits['train'].num_examples\n","#tf.cast converts a variable into given data type\n","# let's cast this number to an integer, as a float may cause an error along the way\n","num_validation_samples=tf.cast(num_validation_samples,tf.int64)\n","# let's also store the number of test samples in a dedicated variable \n","num_test_samples = mnist_info.splits['test'].num_examples\n","# once more, we'd prefer an integer (rather than the default float)\n","num_test_samples = tf.cast(num_test_samples, tf.int64)\n","#we would like to scale our data in such a way so that results are more convinient and numerically stabe (e.g. inputs between 0 and 1)\n","# let's define a function called: scale, that will take an MNIST image and its label\n","def scale(image,label):\n","    #lets make sure that all the values are float as int will be only 0 and 1 \n","    image=tf.cast(image,tf.float32)\n","    #now we scale it so we know mnist images contain value from0 to 255 representing the 256 shaes of gray so if we divide the values by 255 we will get the elements in  floats between 0 an 1\n","    image/=255.\n","    #the dot at the end signifies that we want the result to be in float\n","    return image,label\n","#map function allows a custom transformation on a given dataset  it can allow tranformation on the dataset that can take an input(image) and label and returns image an label\n","#thus we are tranforming the values \n","# we have already decided that we will get the validation data from mnist_train, so in order to implement map \n","scaled_train_and_validation_data = mnist_train.map(scale)\n","#this will scale our whole train dataset and store it in the new variable \n","#similarly for test data\n","test_data = mnist_test.map(scale)\n","#now lets shuffle the data shuffling the data means keeping the same data in different order so that we can differ properly \n","BUFFER_SIZE=10000\n","#buffer is used when we are dealing with enormus datasets so in that case we cant shuffle the whole dataset at once as we cant fit it all in the memory of computer so this wat \n","#we instruct tensorflow to take 10000 sample as a time shuffle them and then take the next 10000\n","# if BUFFER_SIZE=1 => no shuffling will actually happen\n","# if BUFFER_SIZE >= num samples => shuffling is uniform in one go\n","# BUFFER_SIZE more than 1 and less than total sample it optimizes the computational power of computer\n","shuffled_train_and_validation_data=scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n","#Once we have trained and shuffled the data we can extract the train and validation dataset\n","#we calculated validation =10%train in num validation samples so we apply take method to implement it\n","#creating  a validation dataset\n","validation_data=shuffled_train_and_validation_data.take(num_validation_samples)\n","#lets extract the train data\n","train_data=shuffled_train_and_validation_data.skip(num_validation_samples)\n","#we use mini batch gradient descent to train our model to get best accuracy and speed \n","#Batchsize=1 = Schtocastic Gradient Descent\n","#Batchsize=samples=Gradient Descent of all batches\n","#so we take batch size in between\n","#Through an iterative process, gradient descent refines a set of parameters through use of partial differential equations\n","#Gradient descent is an iterative optimization algorithm used in machine learning to minimize a loss function\n","# if we have 10,000 training samples and the batch size is 1,000, then we will have 10 batches and the algorithm will take the first 1,000 samples and train the network.\n","# Next, it will take the second 1,000 samples and train the network again. This will be repeated until the last batch is processed. This is where one epoch of training is finished.\n","BATCH_SIZE=100\n","#batch method is used to form batches\n","train_data=train_data.batch(BATCH_SIZE)\n","validation_data = validation_data.batch(num_validation_samples)\n","#we do not back propagate in the validation ata we only forward propagate so no need to batch it in validation and test we do forward propogate  in batching we find \n","#the average loss and average accuracy but in validation and testing we want the exact value but the proper way is to batch the validation and test samples as well so we make a batch of \n","#all the samples\n","# batch the test data\n","test_data = test_data.batch(num_test_samples)\n","#the validation data must have the same shape and object property as train and test\n","validation_inputs , validation_targets=next(iter(validation_data))\n","#iter creates an object which can be iterate one element at a time but wont load any data\n","#the fit function requires validation inputs and validation targets to be separated. an since we have taken train and test set from mnist and validation we have created on \n","#our own so we make its shape The reason is that we download the train and test data from the TensorFlow MNIST data provider, while we create our own validation dataset.\n","# Then we use these operations to treat the validation data in the same way as the train and test.\n","#outlining the model\n","#there are 784 nodes of input in input layer (28*28) and 10 op nodes one for each digit (0 to 9) , 2 hiden layers each with 50 nodes\n","#Basically, parameters are the ones that the “model” uses to make predictions etc. For example, the weight coefficients in a linear regression model. \n","#Hyperparameters are the ones that help with the learning process. e.g hidden layer\n"],"metadata":{"id":"QuGlST79TCId","colab":{"base_uri":"https://localhost:8080/","height":188,"referenced_widgets":["0932752fce5a49319a4fac877a04e037","a2d63bff96f8469598c6ac0da79a0439","b66569ab0ba04aa899272dbc9b3130a5","7c737b8e7a7945deab0ecd5fbb0bda72","9cdd318cece241968acfd7f078f1941f","fd8e71368ce74000a7ae3da016e07cbb","3c1c849ea964488bb93624bf46b95463","6d6d4bbff3be4e639aad58bcaef2f07a","24fd86990ac242959fbed31c6962ace7","fca7ba5851014c27973fe79f6de52fc2","25757877d7dc4941bb6cc0ad6aa06ebb"]},"executionInfo":{"status":"ok","timestamp":1645509480418,"user_tz":-330,"elapsed":5477,"user":{"displayName":"Hiral Gujrathi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04049263581643961458"}},"outputId":"8aa38b07-09d9-4245-a374-ca453568dd59"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n","local data directory. If you'd instead prefer to read directly from our public\n","GCS bucket (recommended if you're running on GCP), you can instead pass\n","`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n","\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0932752fce5a49319a4fac877a04e037","version_minor":0,"version_major":2},"text/plain":["Dl Completed...:   0%|          | 0/4 [00:00<?, ? file/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"]}]},{"cell_type":"code","source":["input_size = 784\n","output_size=10\n","hidden_layer_size=50\n","#keras is used to lay down the model that is to stack layers\n","model=tf.keras.Sequential([\n","                           #first is the input layer and our data is such that each observation is 28*28*1 and we use Flatten function which belongs to layers module in order to transform a tensor \n","                           #into a vector Each image of the mnist data set is of shape (28,28,1). This means that each image has a height of 28 pixels, a width of 28 pixels, \n","                           #and the third dimension is for the black and white color, which ranges from 0 to 255. We want to build a  neural network and feed inputs of images with shape (28, 28, 1) to it. \n","                           #Our neural network (actually the first  layer) gets each input as a vector, so we flatten each input to make a vector. The input shape is (28, 28, 1) and thus the \n","                           #Flatten layer creates a vector of shape (784,) for each input and passes it to the first  layer.\n","                           #    # the first layer (the input layer)\n","    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3\n","    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images\n","                           tf.keras.layers.Flatten(input_shape=(28,28,1)),\n","                           #The most important feature in an activation function is its ability to add non-linearity into a neural network. it makes \n","                           #possible to introduce non linearity in the network otherwise its just a linear model and hidden layers are useless if we dont introduce a activation function (geeks for geeks)\n","                           #relu is one of the activation function that gives 0 if negatvie input is given and returns a positive value when positive value is given as input\n","                           #tf.keras.layers.dense returns the shape of the respective output so after input comes the hidden layer so we get the output of first mathematical operation \n","                           #as the size of hidden layer\n","                           tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n","                           tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n","                           #we want to transform the output values into probabilites thats why we use softmax activation\n","                           #Softmax is a very interesting activation function because it not only maps our output to a [0,1] range but also maps each output in such a way that the total sum is 1.\n","                           #relu has output 0 if the input is less than 0, and raw output otherwise. That is, if the input is greater than 0, the output is equal to the input.\n","                           tf.keras.layers.Dense(output_size,activation='softmax'),\n","\n","                              ])"],"metadata":{"id":"6ZW4CZ-dXwgA","executionInfo":{"status":"ok","timestamp":1645509488299,"user_tz":-330,"elapsed":3,"user":{"displayName":"Hiral Gujrathi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04049263581643961458"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#choosing the optimization an loss function\n","#accuracy calculates the accuracy\n","# Adam optimization is a stochastic gradient descent method it gives a broder view\n","model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n","#Categorical cross-entropy is used when true labels are one-hot encoded, for example, we have the following true values for 3-class classification problem [1,0,0], [0,1,0] and [0,0,1].\n","#In sparse categorical cross-entropy , truth labels are integer encoded, for example, [1], [2] and [3] for 3-class problem.\n","#when you have integer targets instead of categorical vectors as targets, you can use sparse categorical crossentropy we used one hot encoding \n","#in dog cat horse so there catagorical entropy here the dataset is too large\n","#if you use categorical-cross-entropy you need one-hot encoding, and if you use sparse-categorical-cross-entropy you encode as normal integers."],"metadata":{"id":"8pBOKGpaX4XH","executionInfo":{"status":"ok","timestamp":1645509494664,"user_tz":-330,"elapsed":583,"user":{"displayName":"Hiral Gujrathi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04049263581643961458"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#training the model\n","#lets create the variable storing number of epochs that we wanna train \n","NUM_EPOCHS=5\n","model.fit(train_data,epochs=NUM_EPOCHS,validation_data=(validation_inputs,validation_targets),verbose=2)\n","#train_data contains 54,000 samples and batch_size is set to 1000. So, we have 54 train batches and model.fit logs 54/54.\n","#train_data contains 54,000 samples and batch_size is set to 100. So, we have 540 train batches and model.fit logs 540/540 (we can use any)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1TXbe-NbbG8p","executionInfo":{"status":"ok","timestamp":1645509524028,"user_tz":-330,"elapsed":26741,"user":{"displayName":"Hiral Gujrathi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04049263581643961458"}},"outputId":"517272d0-8b9b-4124-a992-57bf47120ff5"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","540/540 - 7s - loss: 0.4036 - accuracy: 0.8869 - val_loss: 0.2135 - val_accuracy: 0.9352 - 7s/epoch - 13ms/step\n","Epoch 2/5\n","540/540 - 3s - loss: 0.1776 - accuracy: 0.9480 - val_loss: 0.1606 - val_accuracy: 0.9517 - 3s/epoch - 6ms/step\n","Epoch 3/5\n","540/540 - 3s - loss: 0.1349 - accuracy: 0.9601 - val_loss: 0.1292 - val_accuracy: 0.9627 - 3s/epoch - 6ms/step\n","Epoch 4/5\n","540/540 - 3s - loss: 0.1096 - accuracy: 0.9673 - val_loss: 0.1111 - val_accuracy: 0.9688 - 3s/epoch - 6ms/step\n","Epoch 5/5\n","540/540 - 3s - loss: 0.0931 - accuracy: 0.9719 - val_loss: 0.0960 - val_accuracy: 0.9730 - 3s/epoch - 6ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f03eca94650>"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["#testing model\n","#if the accuracy is 97% that does not mean that our model is 97% accurate that is the validation accuracy we get the final accuracy from the testing data\n","#we train on training data and validate on the validation data and thats how we find out that our weights and biases dont overfit once we have created a model then we fidddle \n","#with hyperparameters i.e. learning rate etc then we check the accuracy agsin we try to find the best \n","#hyperparameters bu they are not the best hyper parameters they just fit our validation data best by overtuning them we are overfitting \n","#the validation set is considered as a benchmark for how good the model is and test set is the reality check it is the dataset the model has truly never seen\n","#model.evaluate returns the loss value and metrics value for the model in test mode\n","#we get the loss and accuracy so we store them in separate variables\n","test_loss,test_accuracy=model.evaluate(test_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zSW2yCkUCm4E","executionInfo":{"status":"ok","timestamp":1645509527714,"user_tz":-330,"elapsed":792,"user":{"displayName":"Hiral Gujrathi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04049263581643961458"}},"outputId":"a5c3c7c5-90f2-4157-cd81-7167a89a9f3b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 917ms/step - loss: 0.1000 - accuracy: 0.9697\n"]}]},{"cell_type":"code","source":["print('Test loss: {0:.2f}. Test accuracy : {1:.2f}%'.format(test_loss,test_accuracy*100.))\n","#if we get test accuracy as 50% or 60% then we know that our model has overfit and it will fail but getting a value close to validation accuracy it shows we have not overfit \n","#test accuracy shows how our model will work in real world"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jM6cxg9KM8gX","executionInfo":{"status":"ok","timestamp":1645509533051,"user_tz":-330,"elapsed":536,"user":{"displayName":"Hiral Gujrathi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04049263581643961458"}},"outputId":"e3b96b8c-dad1-406d-dff0-dd58b1dba7bd"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Test loss: 0.10. Test accuracy : 96.97%\n"]}]}]}